patcher.py:532: UserWarning: WARNING: One or more of your selected dataset(s) contains at least one netcdf file that does not follow netcdf convections. Less useful netcdf loader had to be used.
  warnings.warn('WARNING: One or more of your selected dataset(s) contains at least one netcdf file that does not follow netcdf convections. Less useful netcdf loader had to be used.')
patcher.py:1158: UserWarning: While generating patches for a single timestep, the function _make_patches ran out of possible patches that meet the set filters' requirements. Continuing search...
  warnings.warn("While generating patches for a single timestep, the function _make_patches ran out of possible patches that meet the set filters' requirements. Continuing search...")
patcher.py:1158: UserWarning: While generating patches for a single timestep, the function _make_patches ran out of possible patches that meet the set filters' requirements. Continuing search...
  warnings.warn("While generating patches for a single timestep, the function _make_patches ran out of possible patches that meet the set filters' requirements. Continuing search...")
patcher.py:1158: UserWarning: While generating patches for a single timestep, the function _make_patches ran out of possible patches that meet the set filters' requirements. Continuing search...
  warnings.warn("While generating patches for a single timestep, the function _make_patches ran out of possible patches that meet the set filters' requirements. Continuing search...")
patcher.py:1158: UserWarning: While generating patches for a single timestep, the function _make_patches ran out of possible patches that meet the set filters' requirements. Continuing search...
  warnings.warn("While generating patches for a single timestep, the function _make_patches ran out of possible patches that meet the set filters' requirements. Continuing search...")
Traceback (most recent call last):
  File "patcher.py", line 1575, in <module>
    patcher.run()
  File "patcher.py", line 406, in run
    self._load_datasets_from_disk()
  File "patcher.py", line 747, in _load_datasets_from_disk
    self.master_xarray_dataset_examples = self._create_master_xarray_dataset(loaded_datasets_examples, loaded_datetimes_examples, dataset_configs_examples)
  File "patcher.py", line 823, in _create_master_xarray_dataset
    dataset_list_to_merge.append(xr.concat(dataset_list, dim="time_dim"))
  File "/home/tgschmidt/sn_env/lib/python3.8/site-packages/xarray/core/concat.py", line 242, in concat
    return f(
  File "/home/tgschmidt/sn_env/lib/python3.8/site-packages/xarray/core/concat.py", line 521, in _dataset_concat
    combined = concat_vars(vars, dim, positions, combine_attrs=combine_attrs)
  File "/home/tgschmidt/sn_env/lib/python3.8/site-packages/xarray/core/variable.py", line 2936, in concat
    return Variable.concat(variables, dim, positions, shortcut, combine_attrs)
  File "/home/tgschmidt/sn_env/lib/python3.8/site-packages/xarray/core/variable.py", line 1887, in concat
    data = duck_array_ops.concatenate(arrays, axis=axis)
  File "/home/tgschmidt/sn_env/lib/python3.8/site-packages/xarray/core/duck_array_ops.py", line 303, in concatenate
    return _concatenate(as_shared_dtype(arrays), axis=axis)
  File "<__array_function__ internals>", line 5, in concatenate
numpy.core._exceptions.MemoryError: Unable to allocate 74.2 MiB for an array with shape (12, 18, 300, 300) and data type float32
slurmstepd: error: acct_gather_profile/influxdb _send_data: curl_easy_perform failed to send data (discarded). Reason: Couldn't resolve host name
